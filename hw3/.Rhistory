}
res_collection[,r] = beta_new #- beta_true
}
boxplot(t(res_collection),main=paste('N=',N),ylab='估计误差')
}
dec = t(g)%*%d
dec
lgst = function(y,X,beta){
-(t(y)%*%X%*%beta - sum(log(1+exp(X%*%beta))) ) # 返回对数似然函数的相反数，使原问题变成一个求最小值的问题
}
Ns = c(200,500,800,1000)
R = 200
beta_true = c(0.5, 1.2, -1) # 真实的beta值
beta_true = as.matrix(beta_true)
opar <- par(mfrow=c(2,2))
set.seed(555)
for (i in 1:length(Ns)){
N = Ns[i]
res_collection = matrix(rep(0,3*200),nrow=3) #用于收集每一轮得到的beta估计值的残差
for (r in 1:R) {  # 进行R轮估计
X1 = 50*as.matrix(rnorm(N));
X2 = 50*as.matrix(rnorm(N));
ones = as.matrix(rep(1,N))
X = cbind(ones,X1,X2)
y = exp(X %*% beta_true)/(1+exp(X%*%beta_true))
y[which(y>0.5),] = 1
y[which(y<=0.5),] = 0 # 真实的y值
beta = c(0.5,1.2,-1)
beta = as.matrix(beta)# 初始的beta值
iter = 0
while (iter <= 100){ # 使用Armijo准则进行阻尼Newton-Raphson迭代
t = exp(X%*%beta)
p = t/(1+t)
W = diag(as.numeric(p*(1-p)))
H = t(X) %*% W %*% X # 海森矩阵
g = -t(X)%*%(y-p) # 梯度
d = -solve(H)%*%g # 下降方向
delta = 0.1
alpha = 0.1 # 起始步长
sigma = 0.5
f_ori = lgst(y,X,beta)
# print(f_ori)
dec = t(g)%*%d
beta_temp = beta + alpha*d
while( lgst(y,X,beta_temp)
>  f_ori + delta*alpha*dec ){ # 加阻尼，选取合适步长
alpha = alpha*sigma
beta_temp = beta + alpha*d
}
beta_new = beta + alpha*d
if ( norm(beta_new-beta,'M') < 1e-5  ) {
# 用无穷范数刻画新旧beta的差异
break}
beta = beta_new
iter = iter + 1
}
res_collection[,r] = beta_new #- beta_true
}
boxplot(t(res_collection),main=paste('N=',N),ylab='估计误差')
}
lgst = function(y,X,beta){
-(t(y)%*%X%*%beta - sum(log(1+exp(X%*%beta))) ) # 返回对数似然函数的相反数，使原问题变成一个求最小值的问题
}
Ns = c(200,500,800,1000)
R = 200
beta_true = c(0.5, 1.2, -1) # 真实的beta值
beta_true = as.matrix(beta_true)
opar <- par(mfrow=c(2,2))
set.seed(555)
for (i in 1:length(Ns)){
N = Ns[i]
res_collection = matrix(rep(0,3*200),nrow=3) #用于收集每一轮得到的beta估计值的残差
for (r in 1:R) {  # 进行R轮估计
X1 = 50*as.matrix(rnorm(N));
X2 = 50*as.matrix(rnorm(N));
ones = as.matrix(rep(1,N))
X = cbind(ones,X1,X2)
y = exp(X %*% beta_true)/(1+exp(X%*%beta_true))
y[which(y>0.5),] = 1
y[which(y<=0.5),] = 0 # 真实的y值
beta = c(0.5,1.2,-1)
beta = as.matrix(beta)# 初始的beta值
iter = 0
while (iter <= 100){ # 使用Armijo准则进行阻尼Newton-Raphson迭代
t = exp(X%*%beta)
p = t/(1+t)
W = diag(as.numeric(p*(1-p)))
H = t(X) %*% W %*% X # 海森矩阵
g = -t(X)%*%(y-p) # 梯度
d = -solve(H)%*%g # 下降方向
delta = 0.1
alpha = 0.1 # 起始步长
sigma = 0.5
f_ori = lgst(y,X,beta)
# print(f_ori)
dec = t(g)%*%d
beta_temp = beta + alpha*d
while( lgst(y,X,beta_temp)
>  f_ori + delta*alpha*dec ){ # 加阻尼，选取合适步长
alpha = alpha*sigma
beta_temp = beta + alpha*d
}
beta_new = beta + alpha*d
if ( norm(beta_new-beta,'M') < 1e-5  ) {
# 用无穷范数刻画新旧beta的差异
break}
beta = beta_new
iter = iter + 1
}
res_collection[,r] = beta_new - beta_true
}
boxplot(t(res_collection),main=paste('N=',N),ylab='估计误差')
}
lgst = function(y,X,beta){
-(t(y)%*%X%*%beta - sum(log(1+exp(X%*%beta))) )
# 返回对数似然函数的相反数，使原问题变成一个求最小值的问题
}
Ns = c(200,500,800,1000)
R = 200
beta_true = c(0.5, 1.2, -1) # 真实的beta值
beta_true = as.matrix(beta_true)
opar <- par(mfrow=c(2,2))
set.seed(555)
for (i in 1:length(Ns)){
N = Ns[i]
res_collection = matrix(rep(0,3*200),nrow=3) #用于收集每一轮得到的beta估计值的残差
for (r in 1:R) {  # 进行R轮估计
X1 = 100*as.matrix(rnorm(N));
X2 = 120*as.matrix(rnorm(N));
ones = as.matrix(rep(1,N))
X = cbind(ones,X1,X2)
y = exp(X %*% beta_true)/(1+exp(X%*%beta_true))
y[which(y>0.5),] = 1
y[which(y<=0.5),] = 0 # 真实的y值
beta = c(0.5,1.2,-1)
beta = as.matrix(beta)# 初始的beta值
iter = 0
while (iter <= 100){ # 使用Armijo准则进行阻尼Newton-Raphson迭代
t = exp(X%*%beta)
p = t/(1+t)
W = diag(as.numeric(p*(1-p)))
H = t(X) %*% W %*% X # 海森矩阵
g = -t(X)%*%(y-p) # 梯度
d = -solve(H)%*%g # 下降方向
delta = 0.1
alpha = 0.1 # 起始步长
sigma = 0.5
f_ori = lgst(y,X,beta)
# print(f_ori)
dec = t(g)%*%d
beta_temp = beta + alpha*d
while( lgst(y,X,beta_temp)
>  f_ori + delta*alpha*dec ){ # 加阻尼，选取合适步长
alpha = alpha*sigma
beta_temp = beta + alpha*d
}
beta_new = beta + alpha*d
if ( norm(beta_new-beta,'M') < 1e-5  ) {
# 用无穷范数刻画新旧beta的差异
break}
beta = beta_new
iter = iter + 1
}
res_collection[,r] = beta_new - beta_true
}
boxplot(t(res_collection),main=paste('N=',N),ylab='估计误差')
}
lgst = function(y,X,beta){
-(t(y)%*%X%*%beta - sum(log(1+exp(X%*%beta))) )
# 返回对数似然函数的相反数，使原问题变成一个求最小值的问题
}
Ns = c(200,500,800,1000)
R = 200
beta_true = c(0.5, 1.2, -1) # 真实的beta值
beta_true = as.matrix(beta_true)
opar <- par(mfrow=c(2,2))
set.seed(555)
for (i in 1:length(Ns)){
N = Ns[i]
res_collection = matrix(rep(0,3*200),nrow=3) #用于收集每一轮得到的beta估计值的残差
for (r in 1:R) {  # 进行R轮估计
X1 = 100*as.matrix(rnorm(N));
X2 = 120*as.matrix(rnorm(N));
ones = as.matrix(rep(1,N))
X = cbind(ones,X1,X2)
y = exp(X %*% beta_true)/(1+exp(X%*%beta_true))
y[which(y>0.5),] = 1
y[which(y<=0.5),] = 0 # 真实的y值
beta = c(0.5,1.2,-1)
beta = as.matrix(beta)# 初始的beta值
iter = 0
while (iter <= 100){ # 使用Armijo准则进行阻尼Newton-Raphson迭代
t = exp(X%*%beta)
p = t/(1+t)
W = diag(as.numeric(p*(1-p)))
H = t(X) %*% W %*% X # 海森矩阵
g = -t(X)%*%(y-p) # 梯度
d = -solve(H)%*%g # 下降方向
delta = 0.1
alpha = 0.1 # 起始步长
sigma = 0.5
f_ori = lgst(y,X,beta)
# print(f_ori)
dec = t(g)%*%d
beta_temp = beta + alpha*d
while( lgst(y,X,beta_temp)
>  f_ori + delta*alpha*dec ){ # 加阻尼，选取合适步长
alpha = alpha*sigma
beta_temp = beta + alpha*d
}
beta_new = beta + alpha*d
if ( norm(beta_new-beta,'M') < 1e-5  ) {
# 用无穷范数刻画新旧beta的差异
break}
beta = beta_new
iter = iter + 1
}
res_collection[,r] = beta_new - beta_true
}
boxplot(t(res_collection),main=paste('N=',N),ylab='估计误差')
}
beta_new
beta
lgst(y,X,beta_temp)
y,X,beta_temp
y
X
lgst = function(y,X,beta){
-(t(y)%*%X%*%beta - sum(log(1+exp(X%*%beta))) )
# 返回对数似然函数的相反数，使原问题变成一个求最小值的问题
}
Ns = c(200,500,800,1000)
R = 200
beta_true = c(0.5, 1.2, -1) # 真实的beta值
beta_true = as.matrix(beta_true)
opar <- par(mfrow=c(2,2))
set.seed(555)
for (i in 1:length(Ns)){
N = Ns[i]
res_collection = matrix(rep(0,3*200),nrow=3) #用于收集每一轮得到的beta估计值的残差
for (r in 1:R) {  # 进行R轮估计
X1 = 10*as.matrix(rnorm(N));
X2 = 10*as.matrix(rnorm(N));
ones = as.matrix(rep(1,N))
X = cbind(ones,X1,X2)
y = exp(X %*% beta_true)/(1+exp(X%*%beta_true))
y[which(y>0.5),] = 1
y[which(y<=0.5),] = 0 # 真实的y值
beta = c(0,0,0)
beta = as.matrix(beta)# 初始的beta值
iter = 0
while (iter <= 100){ # 使用Armijo准则进行阻尼Newton-Raphson迭代
t = exp(X%*%beta)
p = t/(1+t)
W = diag(as.numeric(p*(1-p)))
H = t(X) %*% W %*% X # 海森矩阵
g = -t(X)%*%(y-p) # 梯度
d = -solve(H)%*%g # 下降方向
delta = 0.1
alpha = 0.1 # 起始步长
sigma = 0.5
f_ori = lgst(y,X,beta)
# print(f_ori)
dec = t(g)%*%d
beta_temp = beta + alpha*d
iter2 = 0
while( lgst(y,X,beta_temp)
>  f_ori + delta*alpha*dec && iter2<50){ # 加阻尼，选取合适步长
alpha = alpha*sigma
beta_temp = beta + alpha*d
iter2 = iter2 + 1
}
beta_new = beta + alpha*d
if ( norm(beta_new-beta,'M') < 1e-5  ) {
# 用无穷范数刻画新旧beta的差异
break}
beta = beta_new
iter = iter + 1
}
res_collection[,r] = beta_new - beta_true
}
boxplot(t(res_collection),main=paste('N=',N),ylab='估计误差')
}
lgst = function(y,X,beta){
-(t(y)%*%X%*%beta - sum(log(1+exp(X%*%beta))) )
# 返回对数似然函数的相反数，使原问题变成一个求最小值的问题
}
Ns = c(200,500,800,1000)
R = 200
beta_true = c(0.5, 1.2, -1) # 真实的beta值
beta_true = as.matrix(beta_true)
opar <- par(mfrow=c(2,2))
set.seed(555)
for (i in 1:length(Ns)){
N = Ns[i]
res_collection = matrix(rep(0,3*200),nrow=3) #用于收集每一轮得到的beta估计值的残差
for (r in 1:R) {  # 进行R轮估计
X1 = 10*as.matrix(rnorm(N));
X2 = 10*as.matrix(rnorm(N));
ones = as.matrix(rep(1,N))
X = cbind(ones,X1,X2)
y = exp(X %*% beta_true)/(1+exp(X%*%beta_true))
y[which(y>0.5),] = 1
y[which(y<=0.5),] = 0 # 真实的y值
beta = c(0,0,0)
beta = as.matrix(beta)# 初始的beta值
iter = 0
while (iter <= 100){ # 使用Armijo准则进行阻尼Newton-Raphson迭代
t = exp(X%*%beta)
p = t/(1+t)
W = diag(as.numeric(p*(1-p)))
H = t(X) %*% W %*% X # 海森矩阵
g = -t(X)%*%(y-p) # 梯度
d = -solve(H)%*%g # 下降方向
delta = 0.1
alpha = 0.1 # 起始步长
sigma = 0.5
f_ori = lgst(y,X,beta)
# print(f_ori)
dec = t(g)%*%d
beta_temp = beta + alpha*d
iter2 = 0
while( lgst(y,X,beta_temp)
>  f_ori + delta*alpha*dec && iter2<50){ # 加阻尼，选取合适步长
alpha = alpha*sigma
beta_temp = beta + alpha*d
iter2 = iter2 + 1
}
beta_new = beta + alpha*d
if ( norm(beta_new-beta,'M') < 1e-5  ) {
# 用无穷范数刻画新旧beta的差异
break}
beta = beta_new
iter = iter + 1
}
res_collection[,r] = beta_new - beta_true
}
boxplot(t(res_collection),main=paste('N=',N),ylab='估计误差')
}
knitr::opts_chunk$set(echo = TRUE,warning = F)
lgst = function(y,X,beta){
-(t(y)%*%X%*%beta - sum(log(1+exp(X%*%beta))) )
# 返回对数似然函数的相反数，使原问题变成一个求最小值的凸问题
}
# Ns = c(200,500,800,1000)
Ns = c(200)
R = 200
beta_true = c(0.5, 1.2, -1) # 真实的beta值
beta_true = as.matrix(beta_true)
opar <- par(mfrow=c(1,2))
set.seed(555)
for (i in 1:length(Ns)){
N = Ns[i]
res_collection = matrix(rep(0,3*200),nrow=3) #用于收集每一轮得到的beta估计值的残差
for (r in 1:R) {  # 进行R轮估计
X1 = as.matrix(rnorm(N));
X2 = as.matrix(rnorm(N));
ones = as.matrix(rep(1,N))
X = cbind(ones,X1,X2)
y = exp(X %*% beta_true)/(1+exp(X%*%beta_true))
y[which(y>0.5),] = 1
y[which(y<=0.5),] = 0 # 真实的y值
beta = c(0,0,0)
beta = as.matrix(beta)# 初始的beta值
iter = 0
while (iter <= 100){ # 使用Armijo准则进行阻尼Newton-Raphson迭代
t = exp(X%*%beta)
p = t/(1+t)
W = diag(as.numeric(p*(1-p)))
H = t(X) %*% W %*% X # 海森矩阵
g = -t(X)%*%(y-p) # 梯度
d = -solve(H)%*%g # 下降方向
delta = 0.1
alpha = 0.1 # 起始步长
sigma = 0.1
f_ori = lgst(y,X,beta)
# print(f_ori)
dec = t(g)%*%d
beta_temp = beta + alpha*d
iter2 = 0
while( lgst(y,X,beta_temp)
>  f_ori + delta*alpha*dec && iter2<10){ # 加阻尼，选取合适步长
alpha = alpha*sigma
beta_temp = beta + alpha*d
iter2 = iter2 + 1
}
beta_new = beta + alpha*d
if ( norm(beta_new-beta,'M') < 1e-5  ) {
# 用无穷范数刻画新旧beta的差异
break}
beta = beta_new
iter = iter + 1
}
res_collection[,r] = beta_new - beta_true
}
boxplot(t(res_collection),main=paste('N=',N),ylab='估计误差')
}
lgst = function(y,X,beta){
-(t(y)%*%X%*%beta - sum(log(1+exp(X%*%beta))) )
# 返回对数似然函数的相反数，使原问题变成一个求最小值的凸问题
}
# Ns = c(200,500,800,1000)
Ns = c(200)
R = 200
beta_true = c(0.5, 1.2, -1) # 真实的beta值
beta_true = as.matrix(beta_true)
opar <- par(mfrow=c(1,2))
set.seed(555)
for (i in 1:length(Ns)){
N = Ns[i]
res_collection = matrix(rep(0,3*200),nrow=3) #用于收集每一轮得到的beta估计值的残差
for (r in 1:R) {  # 进行R轮估计
print(r)
X1 = as.matrix(rnorm(N));
X2 = as.matrix(rnorm(N));
ones = as.matrix(rep(1,N))
X = cbind(ones,X1,X2)
y = exp(X %*% beta_true)/(1+exp(X%*%beta_true))
y[which(y>0.5),] = 1
y[which(y<=0.5),] = 0 # 真实的y值
beta = c(0,0,0)
beta = as.matrix(beta)# 初始的beta值
iter = 0
while (iter <= 100){ # 使用Armijo准则进行阻尼Newton-Raphson迭代
t = exp(X%*%beta)
p = t/(1+t)
W = diag(as.numeric(p*(1-p)))
H = t(X) %*% W %*% X # 海森矩阵
g = -t(X)%*%(y-p) # 梯度
d = -solve(H)%*%g # 下降方向
delta = 0.1
alpha = 0.1 # 起始步长
sigma = 0.1
f_ori = lgst(y,X,beta)
# print(f_ori)
dec = t(g)%*%d
beta_temp = beta + alpha*d
iter2 = 0
while( lgst(y,X,beta_temp)
>  f_ori + delta*alpha*dec && iter2<10){ # 加阻尼，选取合适步长
alpha = alpha*sigma
beta_temp = beta + alpha*d
iter2 = iter2 + 1
}
beta_new = beta + alpha*d
if ( norm(beta_new-beta,'M') < 1e-5  ) {
# 用无穷范数刻画新旧beta的差异
break}
beta = beta_new
iter = iter + 1
}
res_collection[,r] = beta_new - beta_true
}
boxplot(t(res_collection),main=paste('N=',N),ylab='估计误差')
}
lgst = function(y,X,beta){
-(t(y)%*%X%*%beta - sum(log(1+exp(X%*%beta))) )
# 返回对数似然函数的相反数，使原问题变成一个求最小值的凸问题
}
# Ns = c(200,500,800,1000)
Ns = c(200)
R = 200
beta_true = c(0.5, 1.2, -1) # 真实的beta值
beta_true = as.matrix(beta_true)
set.seed(555)
for (i in 1:length(Ns)){
N = Ns[i]
res_collection = matrix(rep(0,3*200),nrow=3) #用于收集每一轮得到的beta估计值的残差
for (r in 1:R) {  # 进行R轮估计
print(r)
X1 = as.matrix(rnorm(N));
X2 = as.matrix(rnorm(N));
ones = as.matrix(rep(1,N))
X = cbind(ones,X1,X2)
y = exp(X %*% beta_true)/(1+exp(X%*%beta_true))
y[which(y>0.5),] = 1
y[which(y<=0.5),] = 0 # 真实的y值
beta = c(0,0,0)
beta = as.matrix(beta)# 初始的beta值
iter = 0
while (iter <= 100){ # 使用Armijo准则进行阻尼Newton-Raphson迭代
t = exp(X%*%beta)
p = t/(1+t)
W = diag(as.numeric(p*(1-p)))
H = t(X) %*% W %*% X # 海森矩阵
g = -t(X)%*%(y-p) # 梯度
d = -solve(H)%*%g # 下降方向
delta = 0.1
alpha = 0.1 # 起始步长
sigma = 0.1
f_ori = lgst(y,X,beta)
# print(f_ori)
dec = t(g)%*%d
beta_temp = beta + alpha*d
iter2 = 0
while( lgst(y,X,beta_temp)
>  f_ori + delta*alpha*dec && iter2<10){ # 加阻尼，选取合适步长
alpha = alpha*sigma
beta_temp = beta + alpha*d
iter2 = iter2 + 1
}
beta_new = beta + alpha*d
if ( norm(beta_new-beta,'M') < 1e-5  ) {
# 用无穷范数刻画新旧beta的差异
break}
beta = beta_new
iter = iter + 1
}
res_collection[,r] = beta_new - beta_true
}
boxplot(t(res_collection),main=paste('N=',N),ylab='估计误差')
}
